// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.9
// 	protoc        v3.12.4
// source: tensorflow_serving/apis/predict.proto

package tensorflow_serving

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	"tomerab.com/cam-hub/internal/frame_analyzer/tensorflow/core/framework"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// PredictRequest specifies which TensorFlow model to run, as well as
// how inputs are mapped to tensors and how outputs are filtered before
// returning to user.
type PredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Model Specification. If version is not specified, will use the latest
	// (numerical) version.
	ModelSpec *ModelSpec `protobuf:"bytes,1,opt,name=model_spec,json=modelSpec,proto3" json:"model_spec,omitempty"`
	// Input tensors.
	// Names of input tensor are alias names. The mapping from aliases to real
	// input tensor names is stored in the SavedModel export as a prediction
	// SignatureDef under the 'inputs' field.
	Inputs map[string]*framework.TensorProto `protobuf:"bytes,2,rep,name=inputs,proto3" json:"inputs,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Output filter.
	// Names specified are alias names. The mapping from aliases to real output
	// tensor names is stored in the SavedModel export as a prediction
	// SignatureDef under the 'outputs' field.
	// Only tensors specified here will be run/fetched and returned, with the
	// exception that when none is specified, all tensors specified in the
	// named signature will be run/fetched and returned.
	OutputFilter  []string `protobuf:"bytes,3,rep,name=output_filter,json=outputFilter,proto3" json:"output_filter,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictRequest) Reset() {
	*x = PredictRequest{}
	mi := &file_tensorflow_serving_apis_predict_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictRequest) ProtoMessage() {}

func (x *PredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_tensorflow_serving_apis_predict_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictRequest.ProtoReflect.Descriptor instead.
func (*PredictRequest) Descriptor() ([]byte, []int) {
	return file_tensorflow_serving_apis_predict_proto_rawDescGZIP(), []int{0}
}

func (x *PredictRequest) GetModelSpec() *ModelSpec {
	if x != nil {
		return x.ModelSpec
	}
	return nil
}

func (x *PredictRequest) GetInputs() map[string]*framework.TensorProto {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *PredictRequest) GetOutputFilter() []string {
	if x != nil {
		return x.OutputFilter
	}
	return nil
}

// Response for PredictRequest on successful run.
type PredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Effective Model Specification used to process PredictRequest.
	ModelSpec *ModelSpec `protobuf:"bytes,2,opt,name=model_spec,json=modelSpec,proto3" json:"model_spec,omitempty"`
	// Output tensors.
	Outputs       map[string]*framework.TensorProto `protobuf:"bytes,1,rep,name=outputs,proto3" json:"outputs,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictResponse) Reset() {
	*x = PredictResponse{}
	mi := &file_tensorflow_serving_apis_predict_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictResponse) ProtoMessage() {}

func (x *PredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_tensorflow_serving_apis_predict_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictResponse.ProtoReflect.Descriptor instead.
func (*PredictResponse) Descriptor() ([]byte, []int) {
	return file_tensorflow_serving_apis_predict_proto_rawDescGZIP(), []int{1}
}

func (x *PredictResponse) GetModelSpec() *ModelSpec {
	if x != nil {
		return x.ModelSpec
	}
	return nil
}

func (x *PredictResponse) GetOutputs() map[string]*framework.TensorProto {
	if x != nil {
		return x.Outputs
	}
	return nil
}

var File_tensorflow_serving_apis_predict_proto protoreflect.FileDescriptor

const file_tensorflow_serving_apis_predict_proto_rawDesc = "" +
	"\n" +
	"%tensorflow_serving/apis/predict.proto\x12\x12tensorflow.serving\x1a&tensorflow/core/framework/tensor.proto\x1a#tensorflow_serving/apis/model.proto\"\x8f\x02\n" +
	"\x0ePredictRequest\x12<\n" +
	"\n" +
	"model_spec\x18\x01 \x01(\v2\x1d.tensorflow.serving.ModelSpecR\tmodelSpec\x12F\n" +
	"\x06inputs\x18\x02 \x03(\v2..tensorflow.serving.PredictRequest.InputsEntryR\x06inputs\x12#\n" +
	"\routput_filter\x18\x03 \x03(\tR\foutputFilter\x1aR\n" +
	"\vInputsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12-\n" +
	"\x05value\x18\x02 \x01(\v2\x17.tensorflow.TensorProtoR\x05value:\x028\x01\"\xf0\x01\n" +
	"\x0fPredictResponse\x12<\n" +
	"\n" +
	"model_spec\x18\x02 \x01(\v2\x1d.tensorflow.serving.ModelSpecR\tmodelSpec\x12J\n" +
	"\aoutputs\x18\x01 \x03(\v20.tensorflow.serving.PredictResponse.OutputsEntryR\aoutputs\x1aS\n" +
	"\fOutputsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12-\n" +
	"\x05value\x18\x02 \x01(\v2\x17.tensorflow.TensorProtoR\x05value:\x028\x01B\x19Z\x14./tensorflow_serving\xf8\x01\x01b\x06proto3"

var (
	file_tensorflow_serving_apis_predict_proto_rawDescOnce sync.Once
	file_tensorflow_serving_apis_predict_proto_rawDescData []byte
)

func file_tensorflow_serving_apis_predict_proto_rawDescGZIP() []byte {
	file_tensorflow_serving_apis_predict_proto_rawDescOnce.Do(func() {
		file_tensorflow_serving_apis_predict_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_tensorflow_serving_apis_predict_proto_rawDesc), len(file_tensorflow_serving_apis_predict_proto_rawDesc)))
	})
	return file_tensorflow_serving_apis_predict_proto_rawDescData
}

var file_tensorflow_serving_apis_predict_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
var file_tensorflow_serving_apis_predict_proto_goTypes = []any{
	(*PredictRequest)(nil),        // 0: tensorflow.serving.PredictRequest
	(*PredictResponse)(nil),       // 1: tensorflow.serving.PredictResponse
	nil,                           // 2: tensorflow.serving.PredictRequest.InputsEntry
	nil,                           // 3: tensorflow.serving.PredictResponse.OutputsEntry
	(*ModelSpec)(nil),             // 4: tensorflow.serving.ModelSpec
	(*framework.TensorProto)(nil), // 5: tensorflow.TensorProto
}
var file_tensorflow_serving_apis_predict_proto_depIdxs = []int32{
	4, // 0: tensorflow.serving.PredictRequest.model_spec:type_name -> tensorflow.serving.ModelSpec
	2, // 1: tensorflow.serving.PredictRequest.inputs:type_name -> tensorflow.serving.PredictRequest.InputsEntry
	4, // 2: tensorflow.serving.PredictResponse.model_spec:type_name -> tensorflow.serving.ModelSpec
	3, // 3: tensorflow.serving.PredictResponse.outputs:type_name -> tensorflow.serving.PredictResponse.OutputsEntry
	5, // 4: tensorflow.serving.PredictRequest.InputsEntry.value:type_name -> tensorflow.TensorProto
	5, // 5: tensorflow.serving.PredictResponse.OutputsEntry.value:type_name -> tensorflow.TensorProto
	6, // [6:6] is the sub-list for method output_type
	6, // [6:6] is the sub-list for method input_type
	6, // [6:6] is the sub-list for extension type_name
	6, // [6:6] is the sub-list for extension extendee
	0, // [0:6] is the sub-list for field type_name
}

func init() { file_tensorflow_serving_apis_predict_proto_init() }
func file_tensorflow_serving_apis_predict_proto_init() {
	if File_tensorflow_serving_apis_predict_proto != nil {
		return
	}
	file_tensorflow_serving_apis_model_proto_init()
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_tensorflow_serving_apis_predict_proto_rawDesc), len(file_tensorflow_serving_apis_predict_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   4,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_tensorflow_serving_apis_predict_proto_goTypes,
		DependencyIndexes: file_tensorflow_serving_apis_predict_proto_depIdxs,
		MessageInfos:      file_tensorflow_serving_apis_predict_proto_msgTypes,
	}.Build()
	File_tensorflow_serving_apis_predict_proto = out.File
	file_tensorflow_serving_apis_predict_proto_goTypes = nil
	file_tensorflow_serving_apis_predict_proto_depIdxs = nil
}
